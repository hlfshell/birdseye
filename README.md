# birdseye

birdseye is a deep neural net that takes images from a self driving car's camera rig and generates an overhead view and semantic mapping of the surrounding area.


# Build Log

## Inspiration

The idea behind the project - given an arbitray camera rig (in our case, front, rear, and side mirror cameras) - create an overhead view and possibly semantic segmentation mapping of the area around the car. The project is my first try at a fully custom generative model.

# CARLA

I had just finished a project creating a semantic segmentation network for self-driving cars; to do that I had created a pretty good set automation tools for CARLA via its Python API. I quickly adapted those to this project and had in the first few hours a good set of scripts to setup maps with random weathers, populate them with cars and pedestrians, and record our hero car (a Mini Cooper) from the viewpoints of its supposed camera rig and its birdseye view. Creating the dataset was just letting the machine run the simulator while I did other things.

## Why not homography?

Homography can certainly solve parts of this problem, and would likely be better for a real world implementation. Homography, however, fails to extend a certain understanding of object's representations and heuristics onto its generation (a car is typically this thick, rectangular, sidewalks are this shape, trees are this shape, etc).

## Approach.

The initial idea - I can take something off the shelf like ResNet50 and slice off the connected layers for each input image, concat those, then just try to build a model to output an image. This... didn't work out too well.

IMAGES HERE

I had naively frozen the ResNet50 layers, thinking that maybe the pretrained classifiers could be used similar to transfer learning. Unfortunately it seemed that this was wishful thinking - the ResNet50 model was definitely not accurately encapsulating the important bits of of what each camera was recording.

## Encoder/Decoder Setup

This led me into trying an encoder/decoder setup. The idea - if an encoder network could create a vector representing what it saw from its vantage point, and a decoder network could reasonably recreate the image, then I could utilize these encoders as the input to my overhead network and be reasonably comortable that they can represent what they see.

My first attempt at this was naive, but still showed some promise. Same approach - ResNet50 head, basic decoder setup.

IMAGES HERE

I should note that this was done prior to building any data augmentation, and I purposely selected a subset (about 10%) of my dataset, and cut out validation testing as well. The idea behind this - if I couldn't get decent results on a net that I was essentially forcing to overfit, then I needed to make deep changes to my approach.

While I could clearly get a decent result here, the blurriness didn't really go away even with many times more epochs. This led me to my biggest change, and where a lot of time was spent. I noticed that my chosen loss function - mean squared error - would eventually find the loss to be low, resulting in a blurry picture that didn't quite create a loss large enough to affect enough of a change in the network. So naturally I thought - what if we could have the loss function continue to punish our blurriness?

## GAN Discriminator Loss

Enter the idea for a discriminator loss. If I treated the encoder/decoder as the generator portion of the model, I could combine my original loss function with a loss generated by how "fooled" a discriminator was. Thus began my quick impromptu study of GANs.

After some research, I attempted to follow a setup for WPGAN-GP (Wasserstein GAN + Gradient Penalty). Following books, blogs, and some papers' codebases, I attempted to recreate the setup, but including my weird encoder/decoder setup. Ultimately, I attempted to build a Wasserstein loss, interoplated layers, and a gradient penalty, but had difficulty with several of them.

On this note, I wished several times during this process that I had done this project in PyTorch; I was already breaking from the standard Keras approach of utilizing the simple `fit` function, and I often found myself butting heads with a lot of the "niceties" that Keras offers built in. I may have to admit upfront that this is likely due to my own lack of experience; I certainly learned a lot about what Keras is doing under the hood during training and its components while debugging problems I encountered. Still, I felt like I remember having more direct control in PyTorch, which would have definitely come into play as I built out my custom training loops.

In the end, I had to abandon much of what I attempted with the WPGAN-GP approach save a handful of "lessons learned" from a collection of references. I ended up keeping the `tanh` function for my discriminator and generator outputs - apparently GANs perform better with these. There are also a few design choices - like training your discriminator a handful of times per generator training, how to batch generated vs real samples together, etc - that I kept in the training loop. Interestingly there was still more tons of advice I researched that I could have applied, but felt like I was ultimately going to be stuck in a rabbit hole of experimentation when this is going to be just a piece of my work. I still thought it worthwhile the time, since I expected I would use the lessons learned here when training the final generator for the overhead view.

* Results of GAN training
* Troubles w/ discriminator
* ???